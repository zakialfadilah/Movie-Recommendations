# -*- coding: utf-8 -*-
"""RecommendationSystem_Movie.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PC_fOjOUu9kXGwj-BdnpttC8h4FrWZGf

## Import Library
"""

# Import library

!pip install keras
from google.colab import drive
import zipfile
import os
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow import keras
from keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate
import tensorflow as tf
from keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout
from collections import Counter
from sklearn.metrics.pairwise import linear_kernel
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras import layers
from sklearn.metrics import mean_squared_error
from math import sqrt
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, r2_score

"""Tahap pertama import seluruh library yang dibutuhkan.

## Load data
"""

# 1. Mount Google Drive
drive.mount('/content/drive')

# 2. Path ke file ZIP di Google Drive
zip_path = '/content/drive/MyDrive/dataset_movie.zip'

# 3. Lokasi untuk mengekstrak isi ZIP
extract_path = '/content/dataset_movie'

# 4. Ekstraksi file ZIP
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# 5. Cek hasil ekstraksi
print("Isi folder setelah ekstraksi:")
print(os.listdir(extract_path))

"""Tahap ini load dataset yang diperlukan."""

# Load dataset ke dataframe df_movie & df_rating
df_movie = pd.read_csv('dataset_movie/dataset_movie/movies.csv')
df_rating = pd.read_csv('dataset_movie/dataset_movie/ratings.csv')

"""Load data ke dataframe ke df_movie dan df_rating.

## Data Understanding & EDA
"""

# Cek sample data df_movie
df_movie

"""Menampilkan sample data dari df_movie. Didapatkan di data df_movie memiliki kolom movieId, title dan genres."""

# Cek describe data df_movie

df_movie.describe()

# Cek describe data df_ratings

df_rating.describe()

"""Menampilkan deskripsi dataset, dari nilai minimum, std hingga total data."""

# Cek sample data df_rating
df_rating

"""Menampilkan sample data dari df_rating. Didapatkan di data df_rating memiliki kolom userId, movieId, rating dan timestamp."""

# Cek informasi dari dataset
print(df_movie.info())
print()
print(df_rating.info())

"""cek informasi detail dari dataframe, pada df_movie didapatkan movieId memiliki tipe data integer, title & genres memiliki tipe data string.

pada df_rating= userId, movieId, dan time stamp memiliki tipe data integer, dan rating memiliki tipe data float.
"""

# Cek data null
print(df_movie.isnull().sum())
print(df_rating.isnull().sum())

"""Cek data null, setelah pengecekan tidak didapatkan data null di kedua dataframe."""

# Cek data duplikat
print(df_movie.duplicated().sum())
print(df_rating.duplicated().sum())

"""Cek data duplikat, setelah pengecekan tidak ada data duplikat di kedua dataframe."""

# Cek total unique users dan movies
print("Unique users:", df_rating['userId'].nunique())
print("Unique movies:", df_rating['movieId'].nunique())

"""Didapatkan sebanyak 162541 unique users dan 59047 unique movies dari df_rating.

### Distribusi data
"""

# Distribusi data rating
sns.histplot(df_rating['rating'], bins=10, kde=False)
plt.title('Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

"""Berikut merupakan visualisasi distribusi rating dari dataframe df_rating"""

# Top 10 movies dengan rating terbanyak
rating_counts = df_rating.groupby('movieId')['rating'].count().reset_index()
rating_counts.columns = ['movieId', 'rating_count']

top_rated_movies = rating_counts.sort_values('rating_count', ascending=False).head(10)
top_movies = top_rated_movies.merge(df_movie, on='movieId')

plt.figure(figsize=(10,5))
sns.barplot(x='rating_count', y='title', data=top_movies, palette='viridis')
plt.title('Top 10 Most Rated Movies')
plt.xlabel('Number of Ratings')
plt.ylabel('Movie Title')
plt.tight_layout()
plt.show()

"""berikut merupakan visualisasi top 10 movies yang paling banyak di rate."""

# Distribusi genre populer
df_movie['genres_split'] = df_movie['genres'].str.split('|')

genre_counts = Counter()
df_movie['genres_split'].apply(lambda x: genre_counts.update(x))

# Convert ke DataFrame
df_genres = pd.DataFrame(genre_counts.items(), columns=['genre', 'count']).sort_values(by='count', ascending=False)

plt.figure(figsize=(10,5))
sns.barplot(x='count', y='genre', data=df_genres.head(10), palette='magma')
plt.title('Top 10 Most Common Genres')
plt.xlabel('Count')
plt.ylabel('Genre')
plt.tight_layout()
plt.show()

"""Berikut merupakan top 10 genre terpopuler pada dataframe df_movie.

## Data Preparation

### Content based filtering
"""

# 1. Subset sample 20000 film (karena keterbatasan RAM dalam megelola dataset agar model dapat dijalankan)
df_movie_sample = df_movie.sample(n=20000, random_state=42).reset_index(drop=True)

# 2. Preprocessing genre
df_movie_sample['genres'] = df_movie_sample['genres'].str.replace('|', ' ')

# 3. TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(df_movie_sample['genres'])

# 4. Build reverse index
indices = pd.Series(df_movie_sample.index, index=df_movie_sample['title']).drop_duplicates()

"""Pada tahap ini, dilakukan pembuatan subsample dengan variable baru df_movie_sample untuk mengambil 20.000 data sebagai sample dikarenakan terbatasnya RAM dalam pemrosesan data. Selanjutnya dilakukan preprocessing genre, untuk memisahkan genre pada kolom genre. Lalu dilakukan TF-IDF untuk mengubah data genre yang sudah dipisah menjadi representasi numerik."""

# Menampilkan Subset sample, df_movie_sample
df_movie_sample

"""Menampilkan 20.000 data sample yang digunakan

### Colaborative Filtering
"""

# Subset sample 20000 rating film (karena keterbatasan RAM dalam megelola dataset agar model dapat dijalankan)
df_rating_sample = df_rating.sample(n=20000, random_state=42).copy()

# cek distribusi rating
df_rating_sample['rating'].hist(bins=20)

"""Membuat data sample sebesar 20.000 data menjadi df_rating_sample."""

# Mendapatkan list unik userId dan movieId
user_ids = df_rating_sample['userId'].unique().tolist()
movie_ids = df_rating_sample['movieId'].unique().tolist()

# Mapping user dan movie ke index
user2user_encoded = {x: i for i, x in enumerate(user_ids)}
movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}
userencoded2user = {i: x for x, i in user2user_encoded.items()}
movieencoded2movie = {i: x for x, i in movie2movie_encoded.items()}

# Tambahkan kolom hasil encoding ke dataframe
df_rating_sample['user'] = df_rating_sample['userId'].map(user2user_encoded)
df_rating_sample['movie'] = df_rating_sample['movieId'].map(movie2movie_encoded)

# Jumlah user dan film
num_users = len(user2user_encoded)
num_movies = len(movie2movie_encoded)
min_rating = df_rating_sample['rating'].min()
max_rating = df_rating_sample['rating'].max()

"""Kode ini digunakan untuk melakukan encoding terhadap userId dan movieId dalam dataframe df_rating_sample, agar dapat digunakan dalam model machine learning, khususnya sistem rekomendasi. Pertama, kode mengambil daftar unik userId dan movieId, lalu membuat mapping dari ID asli ke indeks numerik menggunakan enumerate, sehingga tiap pengguna dan film diberi representasi angka yang unik. Mapping sebaliknya juga dibuat untuk keperluan decoding hasil model nantinya. Setelah itu, kolom baru user dan movie ditambahkan ke dataframe dengan hasil mapping tersebut. Terakhir, kode menghitung jumlah total user, jumlah film, serta nilai rating minimum dan maksimum sebagai informasi tambahan yang bisa digunakan dalam pemodelan."""

x = df_rating_sample[['user', 'movie']].values
y = df_rating_sample['rating'].values.astype(np.float32)

# Train/Test split
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

"""Kode ini mempersiapkan data untuk proses pelatihan model dengan mengambil fitur input berupa pasangan user dan movie dalam bentuk array x, serta nilai rating sebagai target y yang dikonversi ke tipe float32 untuk kompatibilitas dengan model machine learning. Selanjutnya, data dibagi menjadi data latih (x_train, y_train) dan data validasi (x_val, y_val) menggunakan fungsi train_test_split dari scikit-learn, dengan proporsi 80% untuk pelatihan dan 20% untuk validasi, serta random_state=42 digunakan untuk memastikan pembagian data yang konsisten (reproducible) setiap kali kode dijalankan.

## Modelling

### Content based filtering
"""

# Hitung cosine similarity antar film
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

# Fungsi rekomendasi berdasarkan title
def get_recommendations_content(title, top_n=10):
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]
    movie_indices = [i[0] for i in sim_scores]
    return df_movie_sample['title'].iloc[movie_indices].tolist()

"""Kode ini digunakan untuk membuat sistem rekomendasi film berbasis konten menggunakan pendekatan cosine similarity. Pertama, dihitung kemiripan antar film menggunakan linear_kernel pada matriks TF-IDF (tfidf_matrix), yang merepresentasikan fitur teks dari deskripsi atau informasi film. Kemudian, fungsi get_recommendations_content menerima input berupa judul film dan akan mencari film lain yang paling mirip berdasarkan nilai cosine similarity. Fungsi ini bekerja dengan mengambil indeks film berdasarkan judul, lalu menghitung dan mengurutkan skor kemiripannya terhadap semua film lain, dan mengambil top_n film paling mirip (selain film itu sendiri). Hasil akhirnya adalah daftar judul film yang direkomendasikan berdasarkan kemiripan konten."""

# Tes fungsi rekomendasi content based filtering
movie_title = 'The Silent Revolution (2018)'
recommendations_content = get_recommendations_content(movie_title)
print(f"Rekomendasi untuk '{movie_title}':")
for i, rec in enumerate(recommendations_content):
    print(f"{i+1}. {rec}")

"""Kode ini digunakan untuk menguji fungsi rekomendasi berbasis konten (get_recommendations_content) dengan memasukkan judul film "The Silent Revolution (2018)" sebagai input. Fungsi ini akan mengembalikan daftar film yang memiliki kemiripan konten tertinggi dengan film ini. Hasil rekomendasi kemudian dicetak ke layar dengan format yang terurut, menampilkan urutan dan judul dari film-film yang direkomendasikan. Tujuannya adalah untuk melihat apakah sistem rekomendasi dapat memberikan saran film yang relevan berdasarkan kemiripan isi.

### Collaborative Filtering

#### RecommenderNet
"""

# Model rekomendasi berbasis pembelajaran mendalam menggunakan embedding dan dot product
class RecommenderNet(Model):
    def __init__(self, num_users, num_movies, embedding_size=50, min_rating=0.5, max_rating=5.0):
        super().__init__()
        self.user_embedding = layers.Embedding(num_users, embedding_size, embeddings_initializer='he_normal')
        self.movie_embedding = layers.Embedding(num_movies, embedding_size, embeddings_initializer='he_normal')
        self.dot = layers.Dot(axes=1)
        self.output_dense = layers.Dense(1, activation='sigmoid')
        self.min_rating = min_rating
        self.max_rating = max_rating

    # Fungsi pemanggilan model
    # Menerima input pasangan user dan movie (dalam bentuk ID), dan menghasilkan prediksi rating
    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        dot_product = self.dot([user_vector, movie_vector])
        scaled_output = self.output_dense(dot_product) * (self.max_rating - self.min_rating) + self.min_rating
        return scaled_output

"""Kode ini mendefinisikan sebuah model rekomendasi berbasis pembelajaran mendalam menggunakan arsitektur RecommenderNet, yang merupakan subclass dari Model di TensorFlow. Model ini menggunakan pendekatan embedding untuk merepresentasikan pengguna dan film ke dalam vektor berdimensi tetap (embedding_size). Dalam metode __init__, dua layer embedding dibuat masing-masing untuk user dan movie, lalu dikombinasikan menggunakan operasi dot product untuk mengukur interaksi di antara keduanya. Hasil dot product kemudian diproses oleh layer Dense dengan aktivasi sigmoid dan diskalakan ke rentang rating sebenarnya, yaitu antara min_rating dan max_rating. Fungsi call menerima input pasangan user dan movie dalam bentuk ID numerik, mengambil representasi vektornya, menghitung kesamaan melalui dot product, dan mengembalikan prediksi rating yang telah diskalakan ke rentang yang sesuai. Model ini cocok digunakan untuk membangun sistem rekomendasi berbasis interaksi antara user dan item.


"""

# Inisialisasi model RecommenderNet
model_recommendernet = RecommenderNet(num_users, num_movies, min_rating=min_rating, max_rating=max_rating)

# Kompilasi model
model_recommendernet.compile(
    loss='mse',
    optimizer=Adam(learning_rate=0.001)
)

# Melatih model
history_rnet = model_recommendernet.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    batch_size=64,
    epochs=10,
    verbose=1
)

"""Kode ini digunakan untuk inisialisasi, kompilasi, dan pelatihan model rekomendasi RecommenderNet yang telah didefinisikan sebelumnya. Pertama, objek model dibuat dengan memberikan jumlah total user dan movie, serta batas minimum dan maksimum rating. Model kemudian dikompilasi menggunakan fungsi loss mean squared error (mse) karena tugasnya adalah regresi (memprediksi nilai rating), dan menggunakan optimizer Adam dengan learning rate sebesar 0.001. Setelah itu, model dilatih menggunakan data latih (x_train, y_train) dan divalidasi dengan data validasi (x_val, y_val) selama 10 epoch dengan ukuran batch 64. Parameter verbose=1 memastikan bahwa proses pelatihan akan menampilkan output kemajuan pelatihan di konsol.

#### NCF
"""

class NCF(Model):
    def __init__(self, num_users, num_movies, min_rating=0.5, max_rating=5.0, embedding_size=50):
        super().__init__()

        # Membuat layer embedding untuk user
        self.user_embedding = layers.Embedding(num_users, embedding_size)

        # Membuat layer embedding untuk movie
        self.movie_embedding = layers.Embedding(num_movies, embedding_size)

        # Menggabungkan vektor user dan movie
        self.concat = layers.Concatenate()

        # Fully connected layer pertama
        self.dense1 = layers.Dense(128, activation='relu')

        # Dropout untuk regularisasi
        self.dropout1 = layers.Dropout(0.3)

        # Fully connected layer kedua
        self.dense2 = layers.Dense(64, activation='relu')

        # Dropout kedua untuk mengurangi overfitting
        self.dropout2 = layers.Dropout(0.3)

        # Output layer dengan aktivasi sigmoid
        self.output_dense = layers.Dense(1, activation='sigmoid')

        # Nilai minimum dan maksimum rating
        self.min_rating = min_rating
        self.max_rating = max_rating

    # Fungsi pemanggilan model
    # Menerima input pasangan user dan movie (dalam bentuk ID), lalu menghasilkan prediksi rating

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        x = self.concat([user_vector, movie_vector])
        x = self.dense1(x)
        x = self.dropout1(x)
        x = self.dense2(x)
        x = self.dropout2(x)
        output = self.output_dense(x) * (self.max_rating - self.min_rating) + self.min_rating
        return output

"""Kode ini mendefinisikan model rekomendasi Neural Collaborative Filtering (NCF) dengan pendekatan pembelajaran mendalam menggunakan subclass Model dari TensorFlow. Model ini memanfaatkan layer embedding untuk mengubah ID pengguna dan film menjadi representasi vektor berdimensi tetap (embedding_size). Vektor pengguna dan film kemudian digabungkan menggunakan Concatenate, lalu diproses melalui dua lapis fully connected layer (Dense) dengan aktivasi ReLU. Untuk mencegah overfitting, disisipkan dua layer Dropout setelah masing-masing Dense layer. Output dari jaringan dilewatkan ke layer Dense dengan aktivasi sigmoid, kemudian diskalakan ke rentang nilai rating sebenarnya (min_rating hingga max_rating). Fungsi call menerima input berupa pasangan ID pengguna dan film, lalu mengembalikannya sebagai prediksi rating. Model NCF ini menggabungkan kekuatan embedding dan jaringan saraf untuk mempelajari interaksi kompleks antara pengguna dan item."""

# Inisialisasi model NCF
# Parameter: jumlah user, jumlah movie, rating minimum & maksimum

model_ncf = NCF(num_users, num_movies, min_rating=min_rating, max_rating=max_rating)
model_ncf.compile(
    loss='mse',
    optimizer=Adam(learning_rate=0.001)
)

# Melatih model dengan data pelatihan
history_ncf = model_ncf.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    batch_size=64,
    epochs=10,
    verbose=1
)

"""Kode ini menginisialisasi dan melatih model Neural Collaborative Filtering (NCF) yang telah didefinisikan sebelumnya. Model NCF dibuat dengan parameter jumlah pengguna dan film, serta batas minimum dan maksimum rating. Setelah itu, model dikompilasi menggunakan fungsi loss mean squared error (karena prediksi berupa nilai rating kontinu) dan optimizer Adam dengan learning rate 0.001. Proses pelatihan dilakukan dengan memanfaatkan data pelatihan (x_train, y_train) dan data validasi (x_val, y_val), selama 10 epoch dengan batch size 64. Parameter verbose=1 memastikan tampilan progres pelatihan muncul di layar. Tujuannya adalah agar model dapat mempelajari hubungan kompleks antara pengguna dan film untuk menghasilkan prediksi rating yang akurat.

## Top N Recommendation RecommenderNet & NFC
"""

def get_top_n_recommendations(model, user_id, n=10, model_type='recommendernet'):
    # Ambil movie yang belum pernah diberi rating oleh user
    user_encoded = user2user_encoded[user_id]
    watched_movie_ids = df_rating_sample[df_rating_sample['userId'] == user_id]['movieId'].tolist()
    watched_movie_encoded = [movie2movie_encoded[m] for m in watched_movie_ids if m in movie2movie_encoded]
    all_movies = set(range(num_movies))
    movies_to_predict = list(all_movies - set(watched_movie_encoded))

    user_array = np.full(len(movies_to_predict), user_encoded)
    movie_array = np.array(movies_to_predict)

    if model_type == 'recommendernet':
        preds = model.predict(np.stack([user_array, movie_array], axis=1)).flatten()
    elif model_type == 'ncf':
        preds = model.predict([user_array, movie_array]).flatten()

    top_indices = preds.argsort()[-n:][::-1]
    top_movie_encoded = movie_array[top_indices]
    top_movie_ids = [movieencoded2movie[i] for i in top_movie_encoded]
    top_titles = df_movie[df_movie['movieId'].isin(top_movie_ids)]['title'].values
    return top_titles

"""Pada bagian ini, fungsi get_top_n_recommendations digunakan untuk menghasilkan rekomendasi film terbaik untuk seorang pengguna berdasarkan model pembelajaran mesin yang telah dilatih. Fungsi ini menerima input berupa model, user_id, jumlah rekomendasi n, dan jenis model ('recommendernet' atau 'ncf'). Pertama, fungsi mengonversi user_id ke format encoded dan mengambil daftar film yang sudah pernah ditonton oleh pengguna tersebut. Lalu, ia menyusun daftar film yang belum ditonton untuk diprediksi. Selanjutnya, pasangan input user dan movie dikonversi menjadi array dan digunakan sebagai input ke model. Model akan menghasilkan prediksi rating untuk setiap film yang belum ditonton, lalu film-film tersebut diurutkan berdasarkan nilai prediksi tertinggi. Akhirnya, fungsi mengembalikan daftar judul film terbaik yang direkomendasikan untuk pengguna tersebut.

## Evaluation
"""

# --- Prediksi ---
y_pred_rec = model_recommendernet.predict(x_val).flatten()
y_pred_ncf = model_ncf.predict(x_val).flatten()

# --- Hitung RMSE ---
rmse_rec = sqrt(mean_squared_error(y_val, y_pred_rec))
rmse_ncf = sqrt(mean_squared_error(y_val, y_pred_ncf))

print(f"RMSE RecommenderNet: {rmse_rec:.4f}")
print(f"RMSE NCF: {rmse_ncf:.4f}")

# --- Visualisasi RMSE ---
plt.figure(figsize=(6, 4))
plt.bar(['RecommenderNet', 'NCF'], [rmse_rec, rmse_ncf], color=['skyblue', 'orange'])
plt.ylabel('RMSE')
plt.title('Perbandingan RMSE Model')
plt.ylim(0, max(rmse_rec, rmse_ncf) + 0.5)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""Berdasarkan hasil evaluasi menggunakan metrik RMSE (Root Mean Squared Error), model RecommenderNet menunjukkan performa yang sedikit lebih baik dibandingkan dengan model NCF . Selisih ini mengindikasikan bahwa prediksi rating dari RecommenderNet lebih mendekati nilai aktual secara rata-rata."""

# --- Plot Training & Validation Loss ---
plt.figure(figsize=(10, 6))

# RecommenderNet
plt.plot(history_rnet.history['loss'], label='RecommenderNet Loss', color='skyblue')
plt.plot(history_rnet.history['val_loss'], label='RecommenderNet Val Loss', color='deepskyblue', linestyle='--')

# NCF
plt.plot(history_ncf.history['loss'], label='NCF Loss', color='orange')
plt.plot(history_ncf.history['val_loss'], label='NCF Val Loss', color='darkorange', linestyle='--')

plt.title('Training & Validation Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""Berdasarkan visualisasi, RecommenderNet memiliki nilai RMSE lebih rendah dibandingkan NCF , grafik validasi menunjukkan bahwa model ini mulai overfit terhadap data pelatihan setelah beberapa epoch. Sebaliknya, NCF menunjukkan performa yang lebih stabil dan konsisten di data validasi."""

# RecommenderNet MSE
final_rnet_train_loss = history_rnet.history['loss'][-1]
final_rnet_val_loss = history_rnet.history['val_loss'][-1]

# NCF MSE
final_ncf_train_loss = history_ncf.history['loss'][-1]
final_ncf_val_loss = history_ncf.history['val_loss'][-1]

print(f"RecommenderNet - Final Training MSE: {final_rnet_train_loss:.4f}")
print(f"RecommenderNet - Final Validation MSE: {final_rnet_val_loss:.4f}")
print(f"NCF - Final Training MSE: {final_ncf_train_loss:.4f}")
print(f"NCF - Final Validation MSE: {final_ncf_val_loss:.4f}")

# Menghitung Mean Absolute Error (MAE) untuk model RecommenderNet dan NCF

mae_rec = mean_absolute_error(y_val, y_pred_rec)
mae_ncf = mean_absolute_error(y_val, y_pred_ncf)

print(f"MAE RecommenderNet: {mae_rec:.4f}")
print(f"MAE NCF: {mae_ncf:.4f}")

"""Meskipun RecommenderNet unggul pada metrik RMSE, model NCF menunjukkan keunggulan pada metrik MAE, yang mengindikasikan bahwa prediksi NCF secara umum lebih stabil dan lebih konsisten terhadap nilai rating aktual, meskipun mungkin kurang presisi terhadap nilai-nilai ekstrem."""

def get_top_n_recommendations(model, user_id_encoded, model_type='recommendernet', top_n=10):
    # Buat DataFrame berisi semua film
    user_movie_df = pd.DataFrame({'movie': list(movie2movie_encoded.values())})
    user_movie_df['user'] = user_id_encoded
    # Prediksi rating
    predictions = model.predict(user_movie_df[['user', 'movie']].values, verbose=0).flatten()
    user_movie_df['predicted_rating'] = predictions

    # Ambil top N prediksi tertinggi
    top_movies = user_movie_df.sort_values('predicted_rating', ascending=False).head(top_n)

    # Decode movie index ke movieId
    movie_id_map = {v: k for k, v in movie2movie_encoded.items()}
    top_movie_ids = top_movies['movie'].map(movie_id_map)

    # Ambil judul dari df_movie_sample
    top_titles = df_movie[df_movie['movieId'].isin(top_movie_ids)]['title'].values

    if len(top_titles) == 0:
      print("⚠️ Tidak ditemukan judul yang cocok. Coba periksa apakah df_movie_sample punya movieId yang sesuai.")


    return top_titles

"""Fungsi get_top_n_recommendations ini bertujuan untuk memberikan rekomendasi film terbaik kepada pengguna berdasarkan model yang sudah dilatih. Fungsi menerima input berupa model, user_id yang sudah dalam bentuk encoded, tipe model, dan jumlah rekomendasi top_n. Pertama, fungsi membuat DataFrame yang berisi semua film dengan kolom user diisi oleh user_id_encoded. Kemudian, model digunakan untuk memprediksi rating dari semua pasangan user-film tersebut, dan hasil prediksi disimpan dalam kolom predicted_rating. Selanjutnya, film-film diurutkan berdasarkan prediksi rating tertinggi dan diambil sebanyak top_n. Indeks film yang direkomendasikan kemudian di-decode kembali menjadi movieId asli, lalu dicocokkan dengan data judul film dalam df_movie untuk diambil judulnya. Jika tidak ada judul yang cocok ditemukan, fungsi akan memberikan peringatan. Fungsi ini memudahkan pengambilan rekomendasi film terbaik secara efisien dengan menggunakan prediksi model."""

# Ambil user  dari encoding
example_user_encoded = 2

top_10_recommendations = get_top_n_recommendations(model_recommendernet, example_user_encoded, model_type='recommendernet', top_n=10)

print("Top 10 rekomendasi film untuk user:", top_10_recommendations)

"""Ini merupakan hasil top 10 rekomendasi film dari user 2 menggunakan RecommenderNet."""

# Ambil user pertama dari encoding
example_user_encoded = 2

top_10_recommendations = get_top_n_recommendations(model_ncf, example_user_encoded, model_type='ncf', top_n=10)

print("Top 10 rekomendasi film untuk user:", top_10_recommendations)

"""Ini merupakan hasil top 10 rekomendasi film dari user 2 menggunakan NCF.

## Conclusion

Dalam proyek pengembangan sistem rekomendasi film ini, saya mengimplementasikan dua pendekatan utama, yaitu Content-Based Filtering dan Collaborative Filtering. Pada pendekatan Collaborative Filtering, saya menguji dua arsitektur model deep learning, yakni RecommenderNet dan Neural Collaborative Filtering (NCF).

Hasil evaluasi menunjukkan bahwa kedua model memiliki performa yang cukup kompetitif, dengan nilai RMSE sebesar 1.0964 untuk RecommenderNet dan 1.1055 untuk NCF. Meskipun RecommenderNet sedikit lebih unggul dalam hal RMSE dan MSE, model NCF menunjukkan performa yang lebih stabil dan konsisten, terutama dalam proses pelatihan dan validasi.

Berdasarkan evaluasi tersebut, saya memilih NCF sebagai model akhir yang digunakan dalam sistem rekomendasi ini karena kestabilannya dalam proses pelatihan dan kemampuannya dalam menghasilkan prediksi yang lebih merata, meskipun memiliki RMSE sedikit lebih tinggi dibandingkan RecommenderNet. Selain itu, nilai MAE yang lebih rendah pada NCF menunjukkan bahwa prediksi model ini secara umum lebih mendekati nilai rating sesungguhnya.

Pendekatan content-based digunakan sebagai pelengkap untuk memberikan rekomendasi yang lebih personal terutama ketika data interaksi pengguna masih terbatas (cold start problem).
"""